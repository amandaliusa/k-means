rng(2016);
data = load('clustering_data.mat');
x = data.x(:,1);
y = data.x(:,2);

%{
a. Implement the K-means algorithm with K = 3, which seems to be a reasonable
number of clusters for this data set. Show the clusters after the
1st, 5th, 10th, and final iteration. This will illustrate how clusters 
evolve while the algorithm is running.
%}

r = datasample(data.x, 3); % Choose representatives at random
r1 = [r(1), r(4)];
r2 = [r(2), r(5)];
r3 = [r(3), r(6)];

counter = 0; % count iterations 
p = zeros(1, 17); % preallocate vector to hold values of objective function
reps = zeros(17, 6); % preallocate vector to hold all representatives
subplt = 0;

while true
    counter = counter + 1; 
    squared_norms = 0; % for calculating objective function
    
    % store representatives
    reps(counter, 1:2) = r1;
    reps(counter, 3:4) = r2; 
    reps(counter, 5:6) = r3; 
    
    % create clusters
    c1 = [];
    c2 = [];
    c3 = [];
    
    for i = 1:600 % loop through each vector 
        
        v = [x(i), y(i)]; % extract each vector 
        
        % calculate distance of vector from each representative
        d = [norm(v-r1), norm(v-r2), norm(v-r3)];
        closest = min(d); % distance to closest representative 
        
        squared_norms = squared_norms + closest^2;
        
        % update clusters 
        if closest == d(1) 
            c1 = vertcat(c1, v);
        elseif closest == d(2)
            c2 = vertcat(c2, v);
        else
            c3 = vertcat(c3, v);
        end
    end
    
    % update representatives 
    r1 = mean(c1(:,1:2));
    r2 = mean(c2(:,1:2));
    r3 = mean(c3(:,1:2));
    
    p(counter) = squared_norms / 600; % store value of objective function
    
    % show results after the first, fifth, and tenth iterations 
    if counter == 1 || counter == 5 || counter == 10
        subplt = subplt + 1;
        if counter == 1
            figure();
        end
        subplot(1,4,subplt);
        plot(c1(:,1), c1(:,2), 'g.', c2(:,1), c2(:,2), 'r.', ...
                c3(:,1), c3(:,2), 'b.')
        legend('Cluster 1', 'Cluster 2', 'Cluster 3');
        title(sprintf('Iteration %d', counter));
    end
    
    % if representatives are the same as generated by the previous 
    % iteration, the algorithm has converged
    if isequal(r1, reps(counter, 1:2)) && ...
            isequal(r2, reps(counter, 3:4))...
            && isequal(r3, reps(counter, 5:6))
        break
    end
end
% generate final plot after algorithm terminates 
subplot(1,4,4);
plot(c1(:,1), c1(:,2), 'g.', c2(:,1), c2(:,2), 'r.', c3(:,1), c3(:,2), ...
    'b.')
title("Final Clustering");
legend('Cluster 1', 'Cluster 2', 'Cluster 3');

%{
b. Show the dynamics of representatives. That is plot the original data 
and, on top of that, plot the K = 3 trajectories of cluster representatives
(plot their positions after each iteration).
%}
  
figure();
plot(x, y, 'k.', reps(:,1), reps(:,2), 'g', reps(:,3), reps(:,4), 'r', ...
    reps(:,5), reps(:,6), 'b');
title("Representative Trajectories");
legend('Original data','Representative 1', 'Representative 2', ...
    'Representative 3');
    
%{
c. Plot the values of the objective function ? versus the iteration number.
What is the minimum value achieved in the last iteration?
%}
    
iterations = [1:counter]; 
figure();
plot(iterations, p)
title("Values of Objective Function vs. Iteration Number");
xlabel("Iteration Number");
ylabel("Objective Function");

disp(p(17)) % minimum value reached in last iteration is p = 3.7420.

%{
d. To compare your implementation with the built-in one, plot the 
clustering your obtained in (a) and the clustering obtained by using 
kmeans.
%}

idx = kmeans(data.x, 3);

% initialize clusters
kmeansc1 = [];
kmeansc2 = [];
kmeansc3 = [];

for i = 1:600 % loop through each vector
    if idx(i) == 1 % sort vectors into cluster 1
        kmeansc1 = vertcat(kmeansc1, data.x(i, 1:2));
    elseif idx(i) == 2 % sort vectors into cluster 2
        kmeansc2 = vertcat(kmeansc2, data.x(i, 1:2));
    else  % sort vectors into cluster 3
        kmeansc3 = vertcat(kmeansc3, data.x(i, 1:2));
    end
end

% plot result of my implementation vs. built-in k-means algorithm 
figure();
subplot(1,2,1);
plot(c1(:,1), c1(:,2), 'g.', c2(:,1), c2(:,2), 'r.', c3(:,1), c3(:,2), ...
    'b.')
title("My Implementation");
legend('Cluster 1', 'Cluster 2', 'Cluster 3');

subplot(1,2,2);
plot(kmeansc1(:,1), kmeansc1(:,2), 'g.', ...
    kmeansc2(:,1), kmeansc2(:,2), 'b.', ...
    kmeansc3(:,1), kmeansc3(:,2), 'r.');
title("Built-in K-means Implementation");
legend('Cluster 1', 'Cluster 2', 'Cluster 3');
   